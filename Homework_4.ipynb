{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 4 Group 11\n",
    "\n",
    "## 1) Does basic house information reflect house's description?\n",
    "In this assignment we will perform a clustering analysis of house announcements in Rome from Immobiliare.it.\n",
    "\n",
    "Let's start preparing the enironment loading the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll scrape some data from the website starting from this url:\n",
    "\n",
    "https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=1\n",
    "\n",
    "In the url we can notice a parameter referring to the pagination of the results, divided in pages. Each of this pages contains 25 announces.\n",
    "\n",
    "In order to reach at least 10.000 announces, we need to scrape at least 400 pages.\n",
    "\n",
    "First we create the function that returns the urls of the announces inside a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_announces(url):\n",
    "    response = get(url)\n",
    "\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    announce_containers = html_soup.find_all('p', class_ = 'titolo text-primary')\n",
    "    \n",
    "    urls = []\n",
    "    \n",
    "    for container in announce_containers:\n",
    "        if \"/nuove_costruzioni/\" not in container.a['href']: \n",
    "            urls.append(container.a['href'])\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a list with all the announces urls we need. We save it in a csv file to avoid scraping all the pages again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_list = []\n",
    "\n",
    "#for i in range(1,450):\n",
    "#    url = 'https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag='\n",
    "#    url_list = url_list + get_announces(url + str(i))\n",
    "\n",
    "#with open('data/url_list.csv', 'w+', newline='') as myfile:\n",
    "#    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#    for url in url_list:\n",
    "#        wr.writerow([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.immobiliare.it/53131931-Vendita-Bi...\n",
       "1    https://www.immobiliare.it/70420586-Vendita-Bi...\n",
       "2    https://www.immobiliare.it/70288308-Vendita-Ap...\n",
       "3    https://www.immobiliare.it/70114826-Vendita-Tr...\n",
       "4    https://www.immobiliare.it/70355074-Vendita-Tr...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list = pd.read_csv('data/url_list.csv', header=None)\n",
    "url_list = url_list[0]\n",
    "url_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the function to extract the info we need from the announce page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    \n",
    "    id = re.findall(r'(\\d+)', url)[0] # Get announce ID parsing the url\n",
    "    \n",
    "    response = get(url)\n",
    "\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data_container = html_soup.find('ul', class_ = 'list-inline list-piped features__list')\n",
    "    \n",
    "    if data_container is not None:\n",
    "    \n",
    "        for item in data_container.children:\n",
    "\n",
    "            # Locate rooms number\n",
    "            if item.find('div', class_= 'features__label') and item.find('div', class_= 'features__label').contents[0] == 'locali':\n",
    "                rooms = item.find('span', class_ = 'text-bold').contents[0]\n",
    "                rooms = re.sub('[^A-Za-z0-9]+', '', rooms)\n",
    "\n",
    "            # Locate surface extension\n",
    "            if item.find('div', class_= 'features__label') and item.find('div', class_= 'features__label').contents[0] == 'superficie':\n",
    "                area = item.find('span', class_ = 'text-bold').contents[0]\n",
    "                area = re.sub('[^A-Za-z0-9]+', '', area)\n",
    "\n",
    "            # Locate bathrooms number    \n",
    "            if item.find('div', class_= 'features__label') and item.find('div', class_= 'features__label').contents[0] == 'bagni':\n",
    "                bathrooms = item.find('span', class_ = 'text-bold').contents[0]\n",
    "                bathrooms = re.sub('[^A-Za-z0-9]+', '', bathrooms)\n",
    "\n",
    "            # Locate floor number    \n",
    "            if item.find('div', class_= 'features__label') and item.find('div', class_= 'features__label').contents[0] == 'piano':\n",
    "                floor = item.find('abbr', class_ = 'text-bold').contents[0]\n",
    "                floor = re.sub('[^A-Za-z0-9]+', '', floor)\n",
    "\n",
    "            # Extract the description\n",
    "            try:\n",
    "                description = html_soup.find('div', class_ = 'col-xs-12 description-text text-compressed').div.contents[0]\n",
    "                description = re.sub('[^a-zA-Z0-9-_*. ]', '', description) # Remove special charachters\n",
    "                description = description.lstrip(' ') # Remove leading blank spaces\n",
    "            except AttributeError:\n",
    "                return False\n",
    "        \n",
    "    try:\n",
    "        return [[id,rooms,area,bathrooms,floor],[id,description]]\n",
    "    except NameError:\n",
    "        return False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data('https://www.immobiliare.it/70355074-Vendita-Trilocale-viale-Cortina-D-Ampezzo-Roma.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate the url list extracting all the data to put them in two dataframes.\n",
    "\n",
    "In order to save execution time for the next runs, we save the two dataframse in two csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.immobiliare.it/53131931-Vendita-Bilocale-viale-Italo-Calvino-Roma.html\n",
      "https://www.immobiliare.it/70420586-Vendita-Bilocale-via-Prenestina-59-Roma.html\n",
      "https://www.immobiliare.it/70288308-Vendita-Appartamento-via-della-Fotografia-Roma.html\n",
      "https://www.immobiliare.it/70114826-Vendita-Trilocale-via-Genserico-Fontana-11-Roma.html\n",
      "https://www.immobiliare.it/70355074-Vendita-Trilocale-viale-Cortina-D-Ampezzo-Roma.html\n",
      "https://www.immobiliare.it/69659060-Vendita-Appartamento-via-Germanico-24-Roma.html\n",
      "https://www.immobiliare.it/66479763-Vendita-Appartamento-via-Sesto-Rufo-42-Roma.html\n",
      "https://www.immobiliare.it/61733354-Vendita-Appartamento-via-Dandolo-Roma.html\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.DataFrame(columns = ['ID','Rooms','Area','Bathrooms','Floor'])\n",
    "\n",
    "description_df = pd.DataFrame(columns = ['ID','Description'])\n",
    "\n",
    "for url in url_list:\n",
    "    \n",
    "    print(url)\n",
    "    while True:\n",
    "        try:\n",
    "            if get_data(url):\n",
    "\n",
    "                # Convert list in dataframe\n",
    "                row_data = pd.np.asarray(get_data(url)[0])\n",
    "                row_data = pd.DataFrame(data=row_data.reshape(1,5), columns= ['ID','Rooms','Area','Bathrooms','Floor'])\n",
    "\n",
    "                # Append results to data dataframe\n",
    "                data_df = data_df.append(row_data)\n",
    "\n",
    "                # Convert list in dataframe\n",
    "                row_description = pd.np.asarray(get_data(url)[1])\n",
    "                row_description = pd.DataFrame(data=row_description.reshape(1,2), columns= ['ID','Description'])\n",
    "\n",
    "                # Append results to description dataframe\n",
    "                description_df = description_df.append(row_description)\n",
    "        except ConnectionError:\n",
    "            print('Connection Error')\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "\n",
    "data_df.to_csv('data/data.csv')\n",
    "description_df.to_csv('data/description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
